{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/un1tz3r0/anythingdiffusion/blob/main/Anything_Diffusion_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hBAjQO1kiEW"
      },
      "source": [
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=fine_tuning_your_own_diffusion_model_using_clip_retrieval_ipynb)\n",
        "\n",
        "# [AnythingDiffusion](https://github.com/un1tz3r0/anythingdiffusion/)\n",
        "#### by [un1tz3r0](https://linktr.ee/un1tz3r0), based on a notebook by [Alex Spirin](https://twitter.com/devdef).\n",
        "\n",
        "A simple colab to fine-tune your very own diffusion models on images from CLIP-retrieval which are nearby a text prompt, and automatically resume training from the last checkpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ8BNzApp_Xk"
      },
      "source": [
        "# Configure\n",
        "\n",
        "Needs 16gb GPU RAM\n",
        "\n",
        "Works in colab pro and on kaggle "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check GPU Status\n",
        "import subprocess\n",
        "simple_nvidia_smi_display = False#@param {type:\"boolean\"}\n",
        "if simple_nvidia_smi_display:\n",
        "    #!nvidia-smi\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "else:\n",
        "    #!nvidia-smi -i 0 -e 0\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "    nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_ecc_note)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sRp0TCVGG4X1",
        "outputId": "e531d389-fcb6-4fb1-b1e2-a26f9c4c5f75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 16 14:44:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "Disabled ECC support for GPU 00000000:00:04.0.\n",
            "All done.\n",
            "Reboot required.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown This is the name of the subdirectory where your custom model snapshots and logs will be dumped during the training:\n",
        "\n",
        "custom_model_name = \"spiraldiffusion\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Everything, including the dataset and the models and model progress and other training output will be on your drive in <tt>Disco_Diffusion/Fine_Tuning/<error>custom_model_name</error>/</tt>\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "M-QtGUPcFsdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ihEN_8wKEXWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtMv2MEzSzjN",
        "outputId": "3098650c-e536-4044-8abe-81882f3bda13",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@markdown Connect with google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-fL3fb8wpxZ",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e74b1cd-8ab4-4253-877c-575732866ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'guided-diffusion-sxela'...\n",
            "remote: Enumerating objects: 154, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 154 (delta 60), reused 48 (delta 43), pack-reused 64\u001b[K\n",
            "Receiving objects: 100% (154/154), 87.56 KiB | 896.00 KiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "/content/guided-diffusion-sxela\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/guided-diffusion-sxela\n",
            "Collecting blobfile>=1.0.5\n",
            "  Downloading blobfile-1.3.1-py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from guided-diffusion==0.0.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from guided-diffusion==0.0.0) (4.64.0)\n",
            "Collecting urllib3~=1.25\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.7/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (3.8.0)\n",
            "Collecting xmltodict~=0.12.0\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->guided-diffusion==0.0.0) (4.1.1)\n",
            "Installing collected packages: xmltodict, urllib3, pycryptodomex, blobfile, guided-diffusion\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Running setup.py develop for guided-diffusion\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.11 which is incompatible.\u001b[0m\n",
            "Successfully installed blobfile-1.3.1 guided-diffusion-0.0.0 pycryptodomex-3.15.0 urllib3-1.26.11 xmltodict-0.12.0\n"
          ]
        }
      ],
      "source": [
        "#@markdown Download and install guided diffusion\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/Sxela/guided-diffusion-sxela\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Define some helpers and create directories\n",
        "\n",
        "import pathlib, subprocess, os, sys, ipykernel\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  is_colab = True\n",
        "except:\n",
        "  is_colab = False\n",
        "\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "def createParent(filepath):\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n",
        "\n",
        "def pipi(*modulestrs):\n",
        "    res = subprocess.run(['pip', 'install', *modulestrs], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def wget(url, outputdir=None, filename=None):\n",
        "    if outputdir != None:\n",
        "      res = subprocess.run(['wget', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    elif filename != None:\n",
        "      res = subprocess.run(['wget', url, '-O', f'{filename}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    else:\n",
        "      res = subprocess.run(['wget', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "google_drive = True\n",
        "\n",
        "\n",
        "if is_colab:\n",
        "    if google_drive is True:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        rootPath = '/content/drive/MyDrive/Disco_Diffusion'\n",
        "    else:\n",
        "        rootPath = '/content'\n",
        "else:\n",
        "    rootPath = os.getcwd()\n",
        "\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "def createParent(filepath):\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n",
        "\n",
        "# set up some folders based on the custom_model_name in the form for this cell...\n",
        "\n",
        "finetuningRoot = f\"{rootPath}/Fine_Tuning/{custom_model_name}\"\n",
        "createPath(f\"{finetuningRoot}\")\n",
        "\n",
        "datasetRoot = f\"{finetuningRoot}/dataset\"\n",
        "createPath(f\"{datasetRoot}\")\n",
        "\n",
        "trainingRoot = f\"{finetuningRoot}/training\"\n",
        "createPath(f\"{trainingRoot}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OMuXyRFen5l",
        "outputId": "c3321c8c-78b0-4249-fc95-cbf6468065c9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7kNS0XQBIY1"
      },
      "source": [
        "# Get images to train on using CLIP-retrieval\n",
        "\n",
        "Generate a dataset from images retrieved by proximity to a text prompt in the CLIP latent space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmGjWRMM9_VY",
        "outputId": "81b02d54-8bbc-4c81-efd9-ed863a15cd67",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset directory: /content/drive/MyDrive/Disco_Diffusion/Fine_Tuning/spiraldiffusion/dataset\n",
            "Dataset already exists, skipping clip-retrieval...\n",
            "Skipped creating dataset from clip-retrieval, found existing dataset in: /content/drive/MyDrive/Disco_Diffusion/Fine_Tuning/spiraldiffusion/dataset/crop\n"
          ]
        }
      ],
      "source": [
        "# create the dataset using clip retrieval and aiohttp/aiomultiprocessing to download in paralell\n",
        "import shlex\n",
        "\n",
        "dataset_text_prompt = \"spiral\" #@param {type: \"string\"}\n",
        "dataset_fetch_size = 5000 #@param {type: \"integer\"}\n",
        "dataset_crop_count = 6000 #@param {type: \"integer\"}\n",
        "force_existing_dataset = False #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown Check `force_existing_dataset` to ignore an existing folder from a previous run and redownload and process the CLIP-retrieval results anyway. If this is not checked, then after the first time this cell runs, it will skip this lengthy process and use the dataset from the previous run.\n",
        "\n",
        "datasetPath = f'{datasetRoot}'\n",
        "if pathlib.Path(datasetPath).exists():\n",
        "  print(f\"Dataset directory: {datasetPath}\\nDataset already exists, skipping clip-retrieval...\")\n",
        "else:\n",
        "  createPath(datasetPath)\n",
        "\n",
        "if (not pathlib.Path(datasetPath+\"/crop\").exists()) or force_existing_dataset: # or len(list(pathlib.Path(datasetPath).iterdir())) < 3':\n",
        "  print(f\"Creating new dataset from clip-retrieval for the prompt: '{dataset_text_prompt}'\")\n",
        "  try:\n",
        "    datasetOutPath=datasetPath+\"/out\"\n",
        "    createPath(datasetOutPath)\n",
        "    createPath(datasetPath+\"/crop\")\n",
        "\n",
        "    pipi(\"click\", \"clip-retrieval\", \"img2dataset\", \"aiomultiprocess\", \"aiohttp\", \"aiofile\")\n",
        "    wget(\"https://gist.githubusercontent.com/un1tz3r0/a18ba5cf48228ca5cabc58d1d556ad0b/raw/76f36ad320b04e6529d6e68f0cfedae7e8542841/clipfetch.py\", filename=\"clipfetch.py\")\n",
        "    wget(\"https://gist.githubusercontent.com/un1tz3r0/bbada1caa66f0e639e9aa74baec53686/raw/dfce021bec5a869551fe5c4912ad7385617a730e/randomcrops.py\", filename=\"randomcrops.py\")\n",
        "\n",
        "    dataset_text_prompt_q = shlex.quote(dataset_text_prompt)\n",
        "    datasetOutPath_q = shlex.quote(datasetOutPath)\n",
        "    !python3 clipfetch.py $dataset_text_prompt_q $datasetOutPath_q --count $dataset_fetch_size --timeout 5 --paralell 25\n",
        "\n",
        "    print(f\"Generating {dataset_crop_count} cropped squares from source images in {datasetOutPath}...\")\n",
        "\n",
        "    import sys\n",
        "    sys.path.append(pathlib.Path(\"./\").absolute())\n",
        "    import randomcrops\n",
        "    randomcrops.randomcrops(datasetPath+\"/out\", datasetPath+\"/crop\", dataset_crop_count, 256, weighting=0.0, withclasses=False, statusinterval=50)\n",
        "\n",
        "    datasetPath = datasetRoot+\"/crop\"\n",
        "    print(f\"Done creating dataset from clip-retrieval in: {datasetPath}\")\n",
        "    #touch(datasetPath+\"/.completed\")\n",
        "  except Exception as err:\n",
        "    import traceback as tb\n",
        "    tb.print_exc(err)\n",
        "    raise err\n",
        "else:\n",
        "  datasetPath = datasetRoot+\"/crop\"\n",
        "  print(f\"Skipped creating dataset from clip-retrieval, found existing dataset in: {datasetPath}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2gIxZhw2me"
      },
      "source": [
        "# <big><big>Fine Tune</big></big>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqBBkqPjqESf"
      },
      "source": [
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apH5i0hTqz1y",
        "outputId": "e4e31bd7-d03e-4d99-83d1-bd170ca04151",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from latest checkpoint found: /content/drive/MyDrive/Disco_Diffusion/Fine_Tuning/spiraldiffusion/training/ema_0.9999_127000.pt\n",
            "/content/guided-diffusion-sxela\n",
            "Logging to /content/drive/MyDrive/Disco_Diffusion/Fine_Tuning/spiraldiffusion/training\n",
            "creating model and diffusion...\n",
            "creating data loader...\n",
            "training...\n",
            "loading model from checkpoint: /content/drive/MyDrive/Disco_Diffusion/Fine_Tuning/spiraldiffusion/training/ema_0.9999_127000.pt...\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#@markdown # Do the run...\n",
        "\n",
        "import shlex\n",
        "\n",
        "def latest_checkpoint(checkpoint_path, default_model, default_model_url):\n",
        "  import pathlib, os\n",
        "  try:\n",
        "    def kf(f):\n",
        "      return f.lstat().st_mtime\n",
        "    f = str(list(sorted(list(pathlib.Path(checkpoint_path).glob(\"ema_0.9999_*.pt\")), key=kf))[-1])\n",
        "    print(f\"Resuming from latest checkpoint found: {f}\")\n",
        "    return f\n",
        "  except Exception as err:\n",
        "    print(f\"Error finding latest checkpoint in {checkpoint_path}: {err}\")\n",
        "    print(f\"Resuming from default pretrained model: {default_model}\")\n",
        "    if not pathlib.Path(default_model).exists():\n",
        "      print(f\"Downloading default pretrained model from: {default_model_url}\")\n",
        "      wget(default_model_url, filename=default_model)\n",
        "      print(f\"Done!\")\n",
        "    else:\n",
        "      print(f\"Default pretrained model found at: {default_model}\")\n",
        "      print(f\"Skipping model download.\")\n",
        "    return default_model\n",
        "\n",
        "\n",
        "#!wget https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt\n",
        "MODEL_FLAGS=\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16\"\n",
        "DIFFUSION_FLAGS=\"--diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --use_scale_shift_norm False\"\n",
        "RESUME_CHECKPOINT=latest_checkpoint(trainingRoot, f\"{trainingRoot}/lsun_uncond_100M_1200K_bs128.pt\", 'https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt')\n",
        "TRAIN_FLAGS=f\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50  --resume_checkpoint {shlex.quote(RESUME_CHECKPOINT)}\"\n",
        "DATASET_PATH=shlex.quote(datasetPath) #change to point to your dataset path \n",
        "%cd /content/guided-diffusion-sxela\n",
        "!OPENAI_LOGDIR=$trainingRoot python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Anything Diffusion: Fine-Tuning Your Own Diffusion Model Using CLIP-Retrieval.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}