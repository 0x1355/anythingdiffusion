{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/un1tz3r0/anythingdiffusion/blob/main/Anything_Diffusion_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hBAjQO1kiEW"
      },
      "source": [
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=fine_tuning_your_own_diffusion_model_using_clip_retrieval_ipynb)\n",
        "\n",
        "# [AnythingDiffusion](https://github.com/un1tz3r0/anythingdiffusion/)\n",
        "#### by [un1tz3r0](https://linktr.ee/un1tz3r0), based on a notebook by [Alex Spirin](https://twitter.com/devdef).\n",
        "\n",
        "A simple colab to fine-tune your very own diffusion models on images from CLIP-retrieval which are nearby a text prompt, and automatically resume training from the last checkpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ8BNzApp_Xk"
      },
      "source": [
        "# Configure\n",
        "\n",
        "Needs 16gb GPU RAM\n",
        "\n",
        "Works in colab pro and on kaggle "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown This is the name of the subdirectory where your custom model snapshots and logs will be dumped during the training:\n",
        "\n",
        "custom_model_name = \"spiraldiffusion\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Everything, including the dataset and the models and model progress and other training output will be on your drive in <tt>Disco_Diffusion/Fine_Tuning/<error>custom_model_name</error>/</tt>\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "M-QtGUPcFsdO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "ihEN_8wKEXWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtMv2MEzSzjN",
        "outputId": "a746686e-d4c5-4b6f-aede-e12a4efb10b9",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@markdown Connect with google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-fL3fb8wpxZ",
        "outputId": "ee9a5377-9fb3-40f4-8988-b603fb305144",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'guided-diffusion-sxela'...\n",
            "remote: Enumerating objects: 154, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 154 (delta 60), reused 48 (delta 43), pack-reused 64\u001b[K\n",
            "Receiving objects: 100% (154/154), 87.56 KiB | 6.74 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "/content/guided-diffusion-sxela\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/guided-diffusion-sxela\n",
            "Collecting blobfile>=1.0.5\n",
            "  Downloading blobfile-1.3.1-py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from guided-diffusion==0.0.0) (1.12.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from guided-diffusion==0.0.0) (4.64.0)\n",
            "Collecting xmltodict~=0.12.0\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting urllib3~=1.25\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 27.2 MB/s \n",
            "\u001b[?25hCollecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 42.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.7/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->guided-diffusion==0.0.0) (4.1.1)\n",
            "Installing collected packages: xmltodict, urllib3, pycryptodomex, blobfile, guided-diffusion\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Running setup.py develop for guided-diffusion\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.11 which is incompatible.\u001b[0m\n",
            "Successfully installed blobfile-1.3.1 guided-diffusion-0.0.0 pycryptodomex-3.15.0 urllib3-1.26.11 xmltodict-0.12.0\n"
          ]
        }
      ],
      "source": [
        "#@markdown Download and install guided diffusion\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/Sxela/guided-diffusion-sxela\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Define some helpers and create directories\n",
        "\n",
        "import pathlib, subprocess, os, sys, ipykernel\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  is_colab = True\n",
        "except:\n",
        "  is_colab = False\n",
        "\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "def createParent(filepath):\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n",
        "\n",
        "def pipi(*modulestrs):\n",
        "    res = subprocess.run(['pip', 'install', *modulestrs], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def wget(url, outputdir=None, filename=None):\n",
        "    if outputdir != None:\n",
        "      res = subprocess.run(['wget', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    elif filename != None:\n",
        "      res = subprocess.run(['wget', url, '-O', f'{filename}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    else:\n",
        "      res = subprocess.run(['wget', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "google_drive = True\n",
        "\n",
        "\n",
        "if is_colab:\n",
        "    if google_drive is True:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        rootPath = '/content/drive/MyDrive/Disco_Diffusion'\n",
        "    else:\n",
        "        rootPath = '/content'\n",
        "else:\n",
        "    rootPath = os.getcwd()\n",
        "\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "def createParent(filepath):\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n",
        "\n",
        "# set up some folders based on the custom_model_name in the form for this cell...\n",
        "\n",
        "finetuningRoot = f\"{rootPath}/Fine_Tuning/{custom_model_name}\"\n",
        "createPath(f\"{finetuningRoot}\")\n",
        "\n",
        "datasetRoot = f\"{finetuningRoot}/dataset\"\n",
        "createPath(f\"{datasetRoot}\")\n",
        "\n",
        "trainingRoot = f\"{finetuningRoot}/training\"\n",
        "createPath(f\"{trainingRoot}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "4OMuXyRFen5l",
        "outputId": "3f4c572d-00e1-4a0e-e762-e45302b75744"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7kNS0XQBIY1"
      },
      "source": [
        "# Get images to train on using CLIP-retrieval\n",
        "\n",
        "Generate a dataset from images retrieved by proximity to a text prompt in the CLIP latent space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OmGjWRMM9_VY",
        "outputId": "c1fc0786-316c-439e-d6b1-ac6499fe6a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset directory: /content/drive/MyDrive/Disco_Diffusion/Fine_Tuning/spiraldiffusion/dataset\n",
            "Dataset already exists, skipping clip-retrieval...\n",
            "Creating new dataset from clip-retrieval for the prompt: 'spiral'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Collecting clip-retrieval\n",
            "  Downloading clip_retrieval-2.34.2-py3-none-any.whl (338 kB)\n",
            "Collecting img2dataset\n",
            "  Downloading img2dataset-1.32.0-py3-none-any.whl (34 kB)\n",
            "Collecting aiomultiprocess\n",
            "  Downloading aiomultiprocess-0.9.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (3.8.1)\n",
            "Collecting aiofile\n",
            "  Downloading aiofile-3.8.1.tar.gz (18 kB)\n",
            "Collecting fsspec==2022.1.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "Collecting flask-cors<4,>=3.0.10\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: torchvision<2,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from clip-retrieval) (0.13.0+cu113)\n",
            "Collecting requests<3,>=2.27.1\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "Requirement already satisfied: prometheus-client<1,>=0.13.1 in /usr/local/lib/python3.7/dist-packages (from clip-retrieval) (0.14.1)\n",
            "Collecting clip-anytorch<3,>=2.3.1\n",
            "  Downloading clip_anytorch-2.4.0-py3-none-any.whl (1.4 MB)\n",
            "Collecting fire<0.5.0,>=0.4.0\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "Collecting wandb<0.13,>=0.12.10\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "Requirement already satisfied: h5py<4,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from clip-retrieval) (3.1.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from clip-retrieval) (1.21.6)\n",
            "Requirement already satisfied: pandas<2,>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from clip-retrieval) (1.3.5)\n",
            "Requirement already satisfied: pyarrow<8,>=6.0.1 in /usr/local/lib/python3.7/dist-packages (from clip-retrieval) (6.0.1)\n",
            "Collecting multilingual-clip<2,>=1.0.10\n",
            "  Downloading multilingual_clip-1.0.10-py3-none-any.whl (20 kB)\n",
            "Collecting sentence-transformers<3,>=2.2.0\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "Requirement already satisfied: torch<2,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from clip-retrieval) (1.12.0+cu113)\n",
            "Collecting flask-restful<1,>=0.3.9\n",
            "  Downloading Flask_RESTful-0.3.9-py2.py3-none-any.whl (25 kB)\n",
            "Collecting open-clip-torch<2.0.0,>=1.0.1\n",
            "  Downloading open_clip_torch-1.3.0-py3-none-any.whl (1.4 MB)\n",
            "Requirement already satisfied: tqdm<5,>=4.62.3 in /usr/local/lib/python3.7/dist-packages (from clip-retrieval) (4.64.0)\n",
            "Collecting flask<3,>=2.0.3\n",
            "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
            "Collecting faiss-cpu<2,>=1.7.2\n",
            "  Downloading faiss_cpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "Collecting webdataset<0.2,>=0.1.103\n",
            "  Downloading webdataset-0.1.103-py3-none-any.whl (47 kB)\n",
            "Collecting autofaiss<3,>=2.9.6\n",
            "  Downloading autofaiss-2.15.1-py3-none-any.whl (69 kB)\n",
            "Collecting dataclasses<1.0.0,>=0.6\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting exifread-nocycle<4,>=3.0.1\n",
            "  Downloading ExifRead_nocycle-3.0.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: albumentations<2,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from img2dataset) (1.2.1)\n",
            "Requirement already satisfied: opencv-python<5,>=4.5.5.62 in /usr/local/lib/python3.7/dist-packages (from img2dataset) (4.6.0.66)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (2.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (1.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (4.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (1.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp) (4.0.2)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations<2,>=1.1.0->img2dataset) (0.18.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations<2,>=1.1.0->img2dataset) (3.13)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations<2,>=1.1.0->img2dataset) (4.6.0.66)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations<2,>=1.1.0->img2dataset) (1.7.3)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations<2,>=1.1.0->img2dataset) (0.0.4)\n",
            "Collecting embedding-reader<2,>=1.2.0\n",
            "  Downloading embedding_reader-1.5.0-py3-none-any.whl (18 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip-anytorch<3,>=2.3.1->clip-retrieval) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire<0.5.0,>=0.4.0->clip-retrieval) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire<0.5.0,>=0.4.0->clip-retrieval) (1.1.0)\n",
            "Collecting Jinja2>=3.0\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Collecting click\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from flask<3,>=2.0.3->clip-retrieval) (4.12.0)\n",
            "Collecting Werkzeug>=2.2.2\n",
            "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "Collecting itsdangerous>=2.0\n",
            "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
            "Collecting aniso8601>=0.82\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from flask-restful<1,>=0.3.9->clip-retrieval) (2022.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py<4,>=3.1.0->clip-retrieval) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask<3,>=2.0.3->clip-retrieval) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->flask<3,>=2.0.3->clip-retrieval) (2.0.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.1.5->clip-retrieval) (2.8.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations<2,>=1.1.0->img2dataset) (1.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.1->clip-retrieval) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.1->clip-retrieval) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.1->clip-retrieval) (2022.6.15)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (1.3.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (2.6.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (0.11.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations<2,>=1.1.0->img2dataset) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations<2,>=1.1.0->img2dataset) (3.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers<3,>=2.2.0->clip-retrieval) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "Collecting PyYAML\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers<3,>=2.2.0->clip-retrieval) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers<3,>=2.2.0->clip-retrieval) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (57.4.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (3.17.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting braceexpand\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting caio~=0.9.0\n",
            "  Downloading caio-0.9.7-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (87 kB)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip-anytorch<3,>=2.3.1->clip-retrieval) (0.2.5)\n",
            "Building wheels for collected packages: fire, sentence-transformers, aiofile, pathtools\n",
            "  Building wheel for fire (setup.py): started\n",
            "  Building wheel for fire (setup.py): finished with status 'done'\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=40f2b333aa8ab50d0df7841f7466b3904bcf44c14b6a7ad97c7613b51e340c22\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "  Building wheel for sentence-transformers (setup.py): started\n",
            "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=80018d35f7abed9ca338a07f20307083d462809a9c984002963250ad7b42f2da\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "  Building wheel for aiofile (setup.py): started\n",
            "  Building wheel for aiofile (setup.py): finished with status 'done'\n",
            "  Created wheel for aiofile: filename=aiofile-3.8.1-py3-none-any.whl size=19450 sha256=c8b80b4f6b717777e7919c0babdfa74db8f4fe481590cb0aa428123506326ccb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/2e/eb/8a071ffd66fead648f4da7c14d7d6e231fc22e214cce2250c5\n",
            "  Building wheel for pathtools (setup.py): started\n",
            "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=45a6652b340d175a2508fcb09ae2835ddffabce61489b1b971a335e27ff78d1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built fire sentence-transformers aiofile pathtools\n",
            "Installing collected packages: smmap, requests, PyYAML, MarkupSafe, gitdb, Werkzeug, tokenizers, shortuuid, setproctitle, sentry-sdk, pathtools, Jinja2, itsdangerous, huggingface-hub, GitPython, fsspec, docker-pycreds, click, braceexpand, webdataset, wandb, transformers, sentencepiece, ftfy, flask, fire, faiss-cpu, exifread-nocycle, embedding-reader, dataclasses, aniso8601, sentence-transformers, open-clip-torch, multilingual-clip, img2dataset, flask-restful, flask-cors, clip-anytorch, caio, autofaiss, clip-retrieval, aiomultiprocess, aiofile\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: itsdangerous\n",
            "    Found existing installation: itsdangerous 1.1.0\n",
            "    Uninstalling itsdangerous-1.1.0:\n",
            "      Successfully uninstalled itsdangerous-1.1.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "Successfully installed GitPython-3.1.27 Jinja2-3.1.2 MarkupSafe-2.1.1 PyYAML-6.0 Werkzeug-2.2.2 aiofile-3.8.1 aiomultiprocess-0.9.0 aniso8601-9.0.1 autofaiss-2.15.1 braceexpand-0.1.7 caio-0.9.7 click-8.1.3 clip-anytorch-2.4.0 clip-retrieval-2.34.2 dataclasses-0.6 docker-pycreds-0.4.0 embedding-reader-1.5.0 exifread-nocycle-3.0.1 faiss-cpu-1.7.2 fire-0.4.0 flask-2.2.2 flask-cors-3.0.10 flask-restful-0.3.9 fsspec-2022.1.0 ftfy-6.1.1 gitdb-4.0.9 huggingface-hub-0.8.1 img2dataset-1.32.0 itsdangerous-2.1.2 multilingual-clip-1.0.10 open-clip-torch-1.3.0 pathtools-0.1.2 requests-2.28.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 sentry-sdk-1.9.3 setproctitle-1.3.1 shortuuid-1.0.9 smmap-5.0.0 tokenizers-0.12.1 transformers-4.21.1 wandb-0.12.21 webdataset-0.1.103\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a871c0f6f621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdataset_text_prompt_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python3 clipfetch.py $dataset_text_prompt_q $datasetOutPath_q --count $dataset_fetch_size --timeout 5 --paralell 25'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'shlex' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a871c0f6f621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mprint_exc\u001b[0;34m(limit, file, chain)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;34m\"\"\"Shorthand for 'print_exception(*sys.exc_info(), limit, file)'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mprint_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mformat_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mprint_exception\u001b[0;34m(etype, value, tb, limit, file, chain)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     for line in TracebackException(\n\u001b[0;32m--> 104\u001b[0;31m             type(value), value, tb, limit=limit).format(chain=chain):\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, exc_type, exc_value, exc_traceback, limit, lookup_lines, capture_locals, _seen)\u001b[0m\n\u001b[1;32m    506\u001b[0m         self.stack = StackSummary.extract(\n\u001b[1;32m    507\u001b[0m             \u001b[0mwalk_tb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlookup_lines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             capture_locals=capture_locals)\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# Capture now to permit freeing resources: only complication is in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m                 \u001b[0mframe_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'NameError' and 'int'"
          ]
        }
      ],
      "source": [
        "# create the dataset using clip retrieval and aiohttp/aiomultiprocessing to download in paralell\n",
        "import shlex\n",
        "\n",
        "dataset_text_prompt = \"spiral\" #@param {type: \"string\"}\n",
        "dataset_fetch_size = 5000 #@param {type: \"integer\"}\n",
        "dataset_crop_count = 6000 #@param {type: \"integer\"}\n",
        "force_existing_dataset = True #@param {type: \"boolean\"}\n",
        "\n",
        "datasetPath = f'{datasetRoot}'\n",
        "if pathlib.Path(datasetPath).exists():\n",
        "  print(f\"Dataset directory: {datasetPath}\\nDataset already exists, skipping clip-retrieval...\")\n",
        "else:\n",
        "  createPath(datasetPath)\n",
        "\n",
        "if (not pathlib.Path(datasetPath).exists()) or force_existing_dataset: # or len(list(pathlib.Path(datasetPath).iterdir())) < 3':\n",
        "  print(f\"Creating new dataset from clip-retrieval for the prompt: '{dataset_text_prompt}'\")\n",
        "  try:\n",
        "    datasetOutPath=datasetPath+\"/out\"\n",
        "    createPath(datasetOutPath)\n",
        "    createPath(datasetPath+\"/crop\")\n",
        "\n",
        "    pipi(\"click\", \"clip-retrieval\", \"img2dataset\", \"aiomultiprocess\", \"aiohttp\", \"aiofile\")\n",
        "    wget(\"https://gist.githubusercontent.com/un1tz3r0/a18ba5cf48228ca5cabc58d1d556ad0b/raw/2c5f96e27ee077c8e218925380829a72770b0dd2/clipfetch.py\", filename=\"clipfetch.py\")\n",
        "    wget(\"https://raw.githubusercontent.com/un1tz3r0/pixelscapes-dataset/main/randomcrops.py\", filename=\"randomcrops.py\")\n",
        "\n",
        "    dataset_text_prompt_q = shlex.quote(dataset_text_prompt)\n",
        "    datasetOutPath_q = shlex.quote(datasetOutPath)\n",
        "    !python3 clipfetch.py $dataset_text_prompt_q $datasetOutPath_q --count $dataset_fetch_size --timeout 5 --paralell 25\n",
        "\n",
        "    print(f\"Generating {dataset_crop_count} cropped squares from source images in {datasetOutPath}...\")\n",
        "\n",
        "    import sys\n",
        "    sys.path.append(pathlib.Path(\"./\").absolute())\n",
        "    import randomcrops\n",
        "    randomcrops.randomcrops(datasetPath+\"/out\", datasetPath+\"/crop\", dataset_crop_count, 256, weighting=0.0, withclasses=False, statusinterval=50)\n",
        "\n",
        "    datasetPath = datasetRoot+\"/crop\"\n",
        "    print(f\"Done creating dataset from clip-retrieval in: {datasetPath}\")\n",
        "    #touch(datasetPath+\"/.completed\")\n",
        "  except Exception as err:\n",
        "    import traceback as tb\n",
        "    tb.print_exc(err)\n",
        "    raise err\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2gIxZhw2me"
      },
      "source": [
        "# <big><big>Fine Tune</big></big>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqBBkqPjqESf"
      },
      "source": [
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "apH5i0hTqz1y",
        "outputId": "952e7b88-a151-4e47-b6fa-7b69b9e2d09d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-63e7293571cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mMODEL_FLAGS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mDIFFUSION_FLAGS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"--diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --use_scale_shift_norm False\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mRESUME_CHECKPOINT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingRoot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{trainingRoot}/lsun_uncond_100M_1200K_bs128.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mTRAIN_FLAGS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50  --resume_checkpoint {shlex.quote(RESUME_CHECKPOINT)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#change to point to your dataset path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainingRoot' is not defined"
          ]
        }
      ],
      "source": [
        "#@markdown # Do the run...\n",
        "\n",
        "import shlex\n",
        "\n",
        "def latest_checkpoint(checkpoint_path, default_model, default_model_url):\n",
        "  import pathlib, os\n",
        "  try:\n",
        "    def kf(f):\n",
        "      return f.lstat().st_mtime\n",
        "    f = str(list(sorted(list(pathlib.Path(checkpoint_path).glob(\"ema_0.9999_*.pt\")), key=kf))[-1])\n",
        "    print(f\"Resuming from latest checkpoint found: {f}\")\n",
        "    return f\n",
        "  except Exception as err:\n",
        "    print(f\"Error finding latest checkpoint in {checkpoint_path}: {err}\")\n",
        "    print(f\"Resuming from default pretrained model: {default_model}\")\n",
        "    if not pathlib.Path(default_model).exists():\n",
        "      print(f\"Downloading default pretrained model from: {default_model_url}\")\n",
        "      wget(default_model_url, filename=default_model)\n",
        "      print(f\"Done!\")\n",
        "    else:\n",
        "      print(f\"Default pretrained model found at: {default_model}\")\n",
        "      print(f\"Skipping model download.\")\n",
        "    return default_model\n",
        "\n",
        "\n",
        "#!wget https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt\n",
        "MODEL_FLAGS=\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16\"\n",
        "DIFFUSION_FLAGS=\"--diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --use_scale_shift_norm False\"\n",
        "RESUME_CHECKPOINT=latest_checkpoint(trainingRoot, f\"{trainingRoot}/lsun_uncond_100M_1200K_bs128.pt\", 'https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt')\n",
        "TRAIN_FLAGS=f\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50  --resume_checkpoint {shlex.quote(RESUME_CHECKPOINT)}\"\n",
        "DATASET_PATH=shlex.quote(datasetPath) #change to point to your dataset path \n",
        "%cd /content/guided-diffusion-sxela\n",
        "!OPENAI_LOGDIR=$trainingRoot python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Anything Diffusion: Fine-Tuning Your Own Diffusion Model Using CLIP-Retrieval.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}